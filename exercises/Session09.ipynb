{"cells": [{"cell_type": "code", "execution_count": null, "id": "762abd98", "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import gaussian_process\nfrom sklearn.gaussian_process import kernels\n\nimport pickle\nfrom urllib.request import urlopen"}, {"cell_type": "markdown", "id": "f7ed677a", "metadata": {"tags": []}, "source": "# Gaussian processes in scikit-learn\n\n## Exercise 9.1: GP posterior\n\nRepeat exercise 8.2 using `GaussianProcessRegressor` from `sklearn.gaussian_process`, only plotting the credibility regions based on the posterior predictive variance (not drawing samples).\n\nSome useful hints:\n\n- As all supervised machine learning methods in `sklearn`, you first have to construct an object from the model class (in this case `GaussianProcessRegressor`), and thereafter train it on data by using its member function `fit()`.\n  To obtain predictions, use the member function `predict()`.\n  To the latter, you will either have to pass `return_std=True` or `return_cov=True` in order to obtain information about the posterior predictive variance.\n- When you construct the model, you have to define a kernel.\n  The kernels are available in `sklearn.gaussian_process.kernel`, where the squared exponential/RBF kernel is available as `RBF`.\n- The function `fit()` automatically optimizes the hyperparameters.\n  To turn that feature off, you have to pass the argument `optimizer=None` to `GaussianProcessRegressor`.\n- To include the measurement noise, you can formulate it as part of the kernel by using the kernel `WhiteKernel`."}, {"cell_type": "code", "execution_count": null, "id": "5627f29c", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "fdc3bb36", "metadata": {"tags": []}, "source": "## Exercise 9.2: Learning hyperparameters\n\nUntil now, we have made GP regression using predefined hyperparameters, such as the lengthscale $\\ell$ and noise variance $\\sigma^2$.\nIn this exercise, we will estimate $\\ell$ and $\\sigma^2$ from the data by maximizing the marginal likelihood (cf. exercise 8.4 and 8.5).\nThat is done automatically by the `fit()` function in scikit-learn.\nUse, as before, the RBF kernel and measurement noise together, this time with the data\n\n$$\n\\mathbf{x} = \\begin{bmatrix}-5 &-3 &0 &0.1 &1 &4.9 &5\\end{bmatrix}^\\mathsf{T} \\,\\text{ and }\\,\n\\mathbf{y} = \\begin{bmatrix}0 &-0.5 &1 &0.7 &0 &1 &0.7\\end{bmatrix}^\\mathsf{T}.\n$$"}, {"cell_type": "markdown", "id": "37ec3592", "metadata": {"tags": []}, "source": "### (a)\n\nYou still have to provide an initial value of the hyperparameters.\nTry $\\ell = 1$ and $\\sigma^2 = 0.1$.\nWhat hyperparameters do you get when optimizing?\nPlot the corresponding mean and credibility regions."}, {"cell_type": "code", "execution_count": null, "id": "e5bd047e", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "66056357", "metadata": {"tags": []}, "source": "### (b)\n\nTry instead to initialize with $\\ell = 10$ and $\\sigma^2 = 1$.\nWhat do you get now?"}, {"cell_type": "code", "execution_count": null, "id": "413686ba", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "e165cbe7", "metadata": {"tags": []}, "source": "### (c)\n\nTry to explain what happens by making a grid over different hyperparameter values, and inspect the marginal likelihood for each point in that grid.\nThe `GaussianProcessRegressor` class has a member function `log_marginal_likelihood()` which you may use.\n(Do not forget to turn off the hyperparameter optimization!)"}, {"cell_type": "code", "execution_count": null, "id": "77827e38", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "ff3a836b", "metadata": {"tags": []}, "source": "## Exercise 9.3: Modeling CO\u2082 levels\n\nThe amount of carbon dioxide in the atmosphere has been measured continuously at the Mauna Loa observatory, Hawaii.\nIn this problem, you should use a Gaussian process to model the data from 1958 to 2003, and see how well that model can be used for predicting the data from 2004-2019.\nThey present their latest data at [their homepage](https://www.esrl.noaa.gov/gmd/ccgg/trends/), but for your convenience you can use the data in the format available [here](https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa).\nYou can load the data with the following code snippet:"}, {"cell_type": "code", "execution_count": null, "id": "65197ba6", "metadata": {"tags": []}, "outputs": [], "source": "# download data\ndata = pickle.load(\n    urlopen(\"https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa\")\n)\n\n# extract observations and test data\nx = data['X'].flatten()\ny = data['Y'].flatten()\nxtest = data['Xtest'].flatten()\nytest = data['Ytest'].flatten()"}, {"cell_type": "markdown", "id": "f474d2be", "metadata": {"tags": []}, "source": "Here, `x` and `y` contain your training data.\n\nStart exploring some simple kernels, and thereafter you may have a look at page 118-122 of the book [Gaussian processes for machine learning](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf) for some inspiration on how to design a more bespoke kernel for this problem."}, {"cell_type": "code", "execution_count": null, "id": "7a347396", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "kernelspec": {"display_name": "Python 3.10.6 ('course-apml-public')", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}, "vscode": {"interpreter": {"hash": "e0466e4bbc1f6f77653f92f7ee99fe375173484495b8b5339e7493ccb72bc580"}}}, "nbformat": 4, "nbformat_minor": 5}