{"cells": [{"cell_type": "code", "execution_count": null, "id": "762abd98", "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nfrom scipy import stats, optimize\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import gaussian_process\nfrom sklearn.gaussian_process import kernels\n\nimport pickle\nfrom urllib.request import urlopen"}, {"cell_type": "markdown", "id": "f7ed677a", "metadata": {"tags": []}, "source": "# Gaussian Processes: Part 2"}, {"cell_type": "markdown", "id": "ff3a836b", "metadata": {"tags": []}, "source": "## Exercise 9.1: Modeling CO\u2082 levels\n\nThe amount of carbon dioxide in the atmosphere has been measured continuously at the Mauna Loa observatory, Hawaii.\nIn this problem, you should use a Gaussian process to model the data from 1958 to 2003, and see how well that model can be used for predicting the data from 2004-2019.\nThey present their latest data at [their homepage](https://www.esrl.noaa.gov/gmd/ccgg/trends/), but for your convenience you can use the data in the format available [here](https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa).\nYou can load the data with the following code snippet:"}, {"cell_type": "code", "execution_count": null, "id": "65197ba6", "metadata": {"tags": []}, "outputs": [], "source": "# download data\ndata = pickle.load(\n    urlopen(\"https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa\")\n)\n\n# extract observations and test data\nx = data['X'].flatten()\ny = data['Y'].flatten()\nxtest = data['Xtest'].flatten()\nytest = data['Ytest'].flatten()"}, {"cell_type": "markdown", "id": "f474d2be", "metadata": {"tags": []}, "source": "Here, `x` and `y` contain your training data.\n\nStart exploring some simple kernels, and thereafter you may have a look at page 118-122 of the book [Gaussian processes for machine learning](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf) for some inspiration on how to design a more bespoke kernel for this problem."}, {"cell_type": "code", "execution_count": null, "id": "7a347396", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "982d42e0", "metadata": {"tags": []}, "source": "## Exercise 9.2: Learning hyperparameters\n\n\nUntil now, we have made GP regression using predefined hyperparameters, such as the lengthscale $\\ell$ and noise variance $\\sigma^2$.\nIn this exercise, we will estimate $\\ell$ and $\\sigma^2$ from the data by maximizing the marginal likelihood.\nThe logarithm of the marginal likelihood for a Gaussian process observed with Gaussian noise is\n\n$$\n\\log p(\\mathbf{y} \\,|\\, \\mathbf{x}; \\ell, \\sigma_f^2, \\sigma^2) = \\log \\mathcal{N}(\\mathbf{y} \\,|\\, 0, \\mathbf{K}_y) = -\\frac{1}{2} \\mathbf{y}^\\mathsf{T} \\mathbf{K}_y^{-1} \\mathbf{y} - \\frac{1}{2} \\log{|\\mathbf{K}_y|} - \\frac{n}{2}\\log{(2\\pi)}\n$$\n\nwhere $\\mathbf{K}_y = \\kappa(\\mathbf{x}, \\mathbf{x}) + \\sigma^2 \\mathbf{I}$."}, {"cell_type": "markdown", "id": "742ccd58", "metadata": {"tags": []}, "source": "### (a)\n\nWrite a function that takes $\\mathbf{x}$, $\\mathbf{y}$, $\\ell$, $\\sigma_f^2$, and $\\sigma^2$ as inputs and produces the logarithm of the marginal likelihood as output for the squared exponential covariance function."}, {"cell_type": "code", "execution_count": null, "id": "48d4a800", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "0d8e31dd", "metadata": {"tags": []}, "source": "### (b)\n\nConsider the same data as before. Use $\\sigma_f^2 = 1$ and $\\sigma^2 = 0$ and compute the logarithm of the marginal likelihood for values of $\\ell$ between 0.1 and 1 and plot it.\nWhat seems to be the maximal value of the marginal likelihood on this interval?\nDo GP regression based on this value of $\\ell$."}, {"cell_type": "code", "execution_count": null, "id": "abc7bc2a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "65d67a3d", "metadata": {"tags": []}, "source": "## Exercise 9.3: Learning hyperparameters II\n\nIn this exercise we investigate a setting where the marginal likelihood has multiple local minima."}, {"cell_type": "markdown", "id": "f26ec3ba", "metadata": {"tags": []}, "source": "### (a)\n\nNow, consider the following data\n\n$$\n\\mathbf{x} = [\u22125, \u22123, 0, 0.1, 1, 4.9, 5]^\\mathsf{T}, \\qquad\n\\mathbf{y} = [0, \u22120.5, 1, 0.7, 0, 1, 0.7]^\\mathsf{T}\n$$\n\nand compute the log marginal likelihood for both $\\ell$ and $\\sigma^2$.\nUse a logarithmic 2D-grid for values of $\\ell$ spanning from $10^{\u22121}$ to $10^2$ and for $\\sigma^2$ spanning from $10^{\u22122}$ to $10^0$.\nVisualize the marginal likelihood on that grid with a contour plot."}, {"cell_type": "code", "execution_count": null, "id": "9663f991", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "4db9e1c5", "metadata": {"tags": []}, "source": "### (b)\n\nFind the hyperparameters $\\ell$ and $\\sigma^2$ that correspond to the maximal marginal likelihood.\nPerform GP regression on the data using these hyperparameters."}, {"cell_type": "code", "execution_count": null, "id": "5a60bf5f", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "92a11275", "metadata": {"tags": []}, "source": "### (d)\n\nPerform GP regression for the hyperparameters that correspond to other possible local optima of the marginal likelihood.\nWhat differences do you see in your posterior?"}, {"cell_type": "code", "execution_count": null, "id": "f3ccecd1", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "fdc3bb36", "metadata": {"tags": []}, "source": "## Exercise 9.4: Learning hyperparameters III\n\nWe repeat Exercise 9.3, but this time we estimate $\\ell$ and $\\sigma^2$ that maximize the marginal likelihood with the `fit()` function in scikit-learn automatically.\nConsider the same data as in Exercise 9.3(a) and use, as before, the RBF kernel and measurement noise together."}, {"cell_type": "markdown", "id": "37ec3592", "metadata": {"tags": []}, "source": "### (a)\n\nYou still have to provide an initial value of the hyperparameters.\nTry $\\ell = 1$ and $\\sigma^2 = 0.1$.\nWhat hyperparameters do you get when optimizing?\nPlot the corresponding mean and credibility regions."}, {"cell_type": "code", "execution_count": null, "id": "e5bd047e", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "66056357", "metadata": {"tags": []}, "source": "### (b)\n\nTry instead to initialize with $\\ell = 10$ and $\\sigma^2 = 1$.\nWhat do you get now?"}, {"cell_type": "code", "execution_count": null, "id": "413686ba", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "e165cbe7", "metadata": {"tags": []}, "source": "### (c)\n\nTry to explain what happens by making a grid over different hyperparameter values, and inspect the marginal likelihood for each point in that grid.\nThe `GaussianProcessRegressor` class has a member function `log_marginal_likelihood()` which you may use.\n(Do not forget to turn off the hyperparameter optimization!)"}, {"cell_type": "code", "execution_count": null, "id": "77827e38", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}, "vscode": {"interpreter": {"hash": "e0466e4bbc1f6f77653f92f7ee99fe375173484495b8b5339e7493ccb72bc580"}}}, "nbformat": 4, "nbformat_minor": 5}