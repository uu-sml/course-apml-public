{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"id": "iRjddwOiWzr5", "tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_openml\nfrom scipy.stats import multivariate_normal\nfrom scipy.special import softmax"}, {"cell_type": "markdown", "metadata": {"id": "wjyOyZDn6rk2", "tags": []}, "source": "# Unsupervised learning"}, {"cell_type": "markdown", "metadata": {"id": "wjyOyZDn6rk2", "tags": []}, "source": "## Exercise 8.1: PCA on MNIST\n\nIn the lectures the principal component analysis (PCA) was introduced as a method for dimensionality reduction and feature extraction, i.e., to condense data by mapping it to a lower dimensional space of the most important features."}, {"cell_type": "markdown", "metadata": {"id": "wjyOyZDn6rk2", "tags": []}, "source": "### Theoretical background\n\nLet\n\\begin{equation*}\n  \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^\\intercal \\\\ \\vdots \\\\\n    \\mathbf{x}_N^\\intercal \\end{bmatrix} \\in \\mathbb{R}^{N \\times D}\n\\end{equation*}\nbe a matrix of $N$ data samples $\\mathbf{x}_n \\in \\mathbb{R}^D$, which are\ncentered around zero.\nWe consider a PCA with $M < D$ components.\n\nTo project the data points $\\mathbf{x}_n$ to the $M$-dimensional space that is\ndefined by the $M$ principal components of $\\mathbf{X}$, the so-called principal\nsubspace of $\\mathbf{X}$, we can use the singular value decomposition of\n$\\mathbf{X}$. Let $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\mathsf{T}$ be the\nsingular value decomposition of the data matrix $\\mathbf{X}$ with the singular\nvalues sorted in descending order.\n\nThen the projection $\\mathbf{z}_n$ of\ndata point $\\mathbf{x}_n$ to the principal subspace of $\\mathbf{X}$ is given by\n\\begin{equation}\n  \\mathbf{z}_n^\\mathsf{T} = \\mathbf{x}_n^\\mathsf{T} \\begin{bmatrix} \\mathbf{v}_1 & \\cdots & \\mathbf{v}_M \\end{bmatrix},\n\\end{equation}\nwhere $\\mathbf{v}_i$ is the $i$th column of matrix $\\mathbf{V}$. The vector\n$\\mathbf{z}_n$ can be seen as an encoding of the data point\n$\\mathbf{x}_n$ in a lower dimensional space that is constructed by the directions\nfor which the data shows the largest variations.\n\nWith the help of the singular value decomposition of $\\mathbf{X}$, we can also compute\nreconstructions (or \"decodings\") $\\tilde{\\mathbf{x}}_n$ from the encodings $\\mathbf{z}_n$ by\n\\begin{equation*}\n  \\tilde{\\mathbf{x}}_n^\\intercal = \\mathbf{z}_n^\\intercal \\begin{bmatrix} \\mathbf{v}^\\intercal_1 \\\\ \\vdots \\\\ \\mathbf{v}^\\intercal_M \\end{bmatrix}.\n\\end{equation*}\nIf we use $M < D$ components, usually we are not able to recover the original data\nfrom the lower dimensional encodings exactly. Ideally, we would like the\nreconstructions to be as good as possible while keeping $M$ as small as possible.\n\n##### Note\n\nThe singular value decomposition of a matrix $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$\nis defined as a factorization of the form $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma}\n\\mathbf{V}^\\mathsf{T}$ where $\\mathbf{U} \\in \\mathbb{R}^{N \\times N}$ and\n$\\mathbf{V} \\in \\mathbb{R}^{D \\times D}$ are orthogonal matrices and\n$\\mathbf{\\Sigma} \\in \\mathbb{R}^{N \\times D}$ is a rectangular diagonal matrix with\nnon-negative numbers on the diagonal. The diagonal entries of $\\mathbf{\\Sigma}$ are\nthe so-called singular values of $\\mathbf{X}$. A common convention is to sort\nthe singular values in descending order, in which case the diagonal matrix $\\mathbf{\\Sigma}$ is uniquely determined by $\\mathbf{X}$."}, {"cell_type": "markdown", "metadata": {"id": "wjyOyZDn6rk2", "tags": []}, "source": "### Introduction\n\nIn this exercise, we perform and analyse PCA of the MNIST image data set. The MNIST data set consists of 60000 training and 10000 test data points. Each data point consists of a grayscale image with 28 $\\times$ 28 pixels of a handwritten digit. The digit has been size-normalized and centered within a fixed-sized image. Each image is also labeled with the digit (0, 1, ..., 8, or 9) it is depicting.\n\nThe following code block loads the MNIST data set."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# load MNIST data from https://www.openml.org/d/554\nimages, labels = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n\n# normalize images to [0, 1]\nimages = images / 255\n\n# convert labels to integers\nlabels = labels.astype(int)\n\n# split into training and test dataset\ntrain_images, train_labels = images[:60_000], labels[:60_000]\ntest_images, test_labels = images[60_000:], labels[60_000:]"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "The images are loaded as vectors $\\mathbf{x} = [x_1 \\dots x_{784}]^\\mathsf{T}$ where each input variable $x_j$ corresponds to one of the $28 \\times 28 = 784$ pixels in the image. The figure below shows the first 100 images in the training data set."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# plot first 100 training images\nfig, axes = plt.subplots(nrows=10, ncols=10, figsize=(10, 10))\nfor image, ax in zip(train_images, axes.flat):\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.grid(False)\n        ax.imshow(image.reshape(28, 28), vmin=0.0, vmax=1.0, cmap='gray_r')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"id": "SMqxXreK5ghh", "tags": []}, "source": "Let\n\\begin{equation*}\n  \\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^\\mathsf{T} \\\\ \\vdots \\\\\n    \\mathbf{x}_{60000}^\\mathsf{T} \\end{bmatrix} \\in \\mathbb{R}^{60000 \\times 784}\n\\end{equation*}\nbe a matrix of the MNIST training data set where each row $\\mathbf{x}_i^\\mathsf{T}$ represents an image of the training data set (all pixels are normalized to the interval $[0, 1]$). This matrix is huge, it contains $60000 \\times 784 = 47040000$ entries!\n\nMany entries in this matrix are zeros (corresponding to white pixels), and in particular the entries corresponding to white pixels close to the margins of an image are probably not informative and not relevant. In this exercise we will study how much information is lost if we compress the MNIST data set to $M = 2$ principal components.\n\nWe define the mean of the training images as\n\\begin{equation*}\n    \\overline{\\mathbf{x}} = \\frac{1}{60000} \\sum_{i=1}^{60000} \\mathbf{x}_i,\n\\end{equation*}\nand hence the centered training images are given by $\\mathbf{X} - \\overline{\\mathbf{x}}^\\mathsf{T}$.\n\nWe compute the singular value decomposition $\\mathbf{X} - \\overline{\\mathbf{x}}^\\mathsf{T} = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\mathsf{T}$ of the centered MNIST training data with the function [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "wkfNjs3XbMyC", "tags": []}, "outputs": [], "source": "# center training images\ntrain_mean = train_images.mean(axis=0)\ntrain_images_centered = train_images - train_mean\n\nU, S, Vt = np.linalg.svd(train_images_centered, full_matrices=False)"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Variable `S` contains the singular values of `train_images_centered`, in descending order."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": []}, "outputs": [], "source": "# four largest singular values\nS[:4]"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Note that `Vt` correspods to $\\mathbf{V}^\\mathsf{T}$, i.e., it is the transpose of $\\mathbf{V}$. We can recover `train_images_centered` from its singular value decomposition `U`, `S`, and `Vt`."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "np.allclose(train_images_centered, U @ np.diag(S) @ Vt)"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": []}, "outputs": [], "source": "# alternative:\nnp.allclose(train_images_centered, (U * S) @ Vt)"}, {"cell_type": "markdown", "metadata": {"id": "g9Jt2du7O4z1", "tags": []}, "source": "### (a)\n\nCompute the two-dimensional encodings of the images in the MNIST training and the test data set in the two-dimensional latent space spanned by the $M = 2$ principal components of the centered training data, the so-called principal subspace.\n\n*Hints*:\n- Make use of `U`, `S`, and/or `Vt`.\n- Remember that the presence of the center $\\overline{\\textbf{x}}$ needs to be accounted for in the test data as well."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "XnM5TzfNxh7f", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "QqA3hX3ybMyZ", "tags": []}, "source": "Generate 2D scatter plots of the encodings that show the clusters for different labels in the latent space. You can make use of the function `plot_encodings` below."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "XnA0MGjBbMyc", "tags": []}, "outputs": [], "source": "# plot `train_encodings` and `test_encodings` with colorcoding of the corresponding labels\ndef plot_encodings(train_encodings, train_labels, test_encodings, test_labels):\n    # create two plots side by side\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\n    # plot encodings of training data\n    ax = axes[0]\n    ax.scatter(\n        train_encodings[:, 0], train_encodings[:, 1],\n        c=train_labels, cmap=plt.cm.tab10, vmin=-0.5, vmax=9.5, alpha=0.7\n    )\n    ax.set_xlabel(\"$z_1$\")\n    ax.set_ylabel(\"$z_2$\")\n    ax.set_title(\"training data\")\n    \n    # plot encodings of test data\n    ax = axes[1]\n    scatter = ax.scatter(\n        test_encodings[:, 0], test_encodings[:, 1],\n        c=test_labels, cmap=plt.cm.tab10, vmin=-0.5, vmax=9.5, alpha=0.7\n    )\n    ax.set_xlabel(\"$z_1$\")\n    ax.set_ylabel(\"$z_2$\")\n    ax.set_title(\"test data\")\n\n    # add colorbar\n    cb = fig.colorbar(scatter, ticks=range(10), ax=axes.ravel().tolist())\n    cb.ax.set_title(\"digit\")\n    \n    return fig"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 406}, "id": "CEC8YzTmxh7h", "outputId": "d0bc5690-6f2b-4140-d1d9-cbf1ebcec0ee", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "QjsGYI6pbMyt", "tags": []}, "source": "### (b)\n\nCompute the reconstructions of the test images by mapping the encodings of the test data from the latent space back to the space of images."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "eNv8RcWWxh7k", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "QjtEPRQnbMyv", "tags": []}, "source": "Plot some test images and their reconstructed counterparts. You can use the function `plot_reconstructions` below. Which digits can be reconstructed and decoded quite well, and which ones seem to be more challenging? Remember that the reconstructions are obtained by considering only the two principal components."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "yaF1Pr4Sxh7k", "tags": []}, "outputs": [], "source": "# plot a grid of random pairs of `originals` and `reconstructions`\ndef plot_reconstructions(originals, reconstructions, labels, num_rows=4, num_cols=2):\n    # indices of displayed samples\n    n = originals.shape[0]\n    indices = np.random.choice(n, size=num_rows*num_cols, replace=False)\n\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 10))\n    for (idx, ax) in zip(indices, axes.flat):\n        # extract original, reconstruction, and label\n        original = originals[idx]\n        reconstruction = reconstructions[idx]\n        label = labels[idx]\n\n        # configure subplot\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.grid(False)\n        ax.set_title(f\"Label: {label}\", fontweight='bold')\n\n        # plot original and reconstructed image in a grid\n        grid = np.ones((32, 62))\n        grid[2:30, 2:30] = original.reshape(28, 28)\n        grid[2:30, 32:60] = reconstruction.reshape(28, 28)\n        ax.imshow(grid, vmin=0.0, vmax=1.0, cmap='gray_r')\n\n    return fig"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 594}, "id": "pVhQvVPmbMyy", "outputId": "71c7cd7f-54d0-4d6e-9a0e-e52ce73ba5cf", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "bNR9-T54xh7l", "tags": []}, "source": "### (c)\n\nThe comparison of the original images and their reconstructions provides us with some intuition for how much information is lost by the compression of the images to the two-dimensional latent space. As a less subjective measure we calculate the average squared reconstruction error\n\\begin{equation*}\n\\mathrm{sqerr} := \\frac{1}{10000} \\sum_{i=1}^{10000} \\|\\mathbf{x}_i - \\tilde{\\mathbf{x}}_i\\|^2_2\n\\end{equation*}\nof the images $\\mathbf{x}_i \\in {[0,1]}^{784}$ and their reconstructions $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{784}$ ($i = 1,\\ldots, 10000$) in the MNIST test data set. An advantage of an objective measure such as the average squared reconstruction error is that it enables us to compare the PCA with other models for dimensionality reduction.\n\nWhat average squared reconstruction error do you get with PCA?"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ho3lBFIyaKg6", "outputId": "f6d2d4ef-d839-4836-dd26-efc035e02ef9", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "kXOdBYQm1R29", "tags": []}, "source": "## Exercise 8.2 - Derivations for probabilistic PCA\n\n\nIn constrast to (regular) PCA, the so-called probabilistic PCA (PPCA) allows a probabilistic interpretation of the principal components. The probabilistic formulation of PCA also allows us to extend the method and alter its underlying assumptions quite easily.\n\nAs in exercise 11.1, let $\\mathbf{x} \\in \\mathbb{R}^D$ represent a data sample that we\nwant to decode from a lower dimensional representation\n$\\mathbf{z} \\in \\mathbb{R}^M$ with $M < D$. The PPCA model assumes that\n$\\mathbf{z}$ is standard normally distributed and $\\mathbf{x}$\ncan be decoded by a noisy linear transformation of $\\mathbf{z}$.\nMathematically, the model is given by\n\\begin{align*}\n  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_D\\right), \\\\\n  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_M),\n\\end{align*}\nwith parameters $\\mathbf{W} \\in \\mathbb{R}^{D \\times M}$,\n$\\boldsymbol{\\mu} \\in \\mathbb{R}^D$, and $\\sigma^2 > 0$.\n[Michael E. Tipping and Christopher M. Bishop show in \"Probabilistic Principal Component Analysis\"](https://www.jstor.org/stable/2680726)\nthat for $\\sigma^2 \\to 0$\nthe model recovers the standard PCA (but the components of $\\mathbf{z}$ might\nbe permuted).\n\nWe assume that the data $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ is identically\nand independently distributed according to the PPCA model. In a maximum\nlikelihood setting, one determines the parameters $\\mathbf{W}$, $\\boldsymbol{\\mu}$,\nand $\\sigma^2$ that maximize the likelihood\n\\begin{equation*}\n  p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N ; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2)\n  = \\prod_{n=1}^N p(\\mathbf{x}_n; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2),\n\\end{equation*}\nor equivalently the log-likelihood\n\\begin{equation*}\n  \\log p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2)\n  = \\sum_{n=1}^N \\log p(\\mathbf{x}_n; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2).\n\\end{equation*}"}, {"cell_type": "markdown", "metadata": {"id": "xQffYFcTy3jI", "tags": []}, "source": "### (a) \n\n*(Pen and paper exercise)*\n\nShow that for the model of the probabilistic PCA\n\\begin{equation*}\n    p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}, \\mathbf{C}),\n\\end{equation*}\nwhere $\\mathbf{C} = \\mathbf{W}\\mathbf{W}^\\mathsf{T} + \\sigma^2 \\mathbf{I}_D$."}, {"cell_type": "markdown", "metadata": {"id": "3UCseyss1Vsu", "tags": []}, "source": "### (b)\n\n*(Pen and paper exercise)*\n\nShow that the distribution of the latent variable $\\mathbf{z}$ conditioned on $\\mathbf{x}$ is Gaussian as well and given by\n\\begin{equation*}\n    p(\\mathbf{z} \\,|\\, \\mathbf{x}) = \\mathcal{N}\\left(\\mathbf{z}; \\mathbf{M}^{-1} \\mathbf{W}^\\mathsf{T} (\\mathbf{x} - \\boldsymbol{\\mu}), \\sigma^2 \\mathbf{M}^{-1} \\right),\n\\end{equation*}\nwhere $\\mathbf{M} = \\mathbf{W}^\\mathsf{T} \\mathbf{W} + \\sigma^2 \\mathbf{I}_M$."}, {"cell_type": "markdown", "metadata": {"id": "GR9ls8C-s_dd", "tags": []}, "source": "## Exercise 8.3: Gaussian Mixture Model\n\nIn this exercise we estimate the parameters of a Gaussian mixture model with the expectation-maximization (EM) algorithm.\n\nA Gaussian mixture model with $m \\geq 1$ components consists of $m$ mixture components $\\mathcal{N}(\\mu_1, \\Sigma_1), \\ldots, \\mathcal{N}(\\mu_m, \\Sigma_m)$ and corresponding non-negative mixture weights $w_1, \\ldots, w_m$ with $\\sum_{i=1}^m w_i = 1$. It has the probability density function\n\\begin{equation*}\n    \\operatorname{GMM}(x; w_1, \\mu_1, \\Sigma_1, \\ldots, w_m, \\mu_m, \\Sigma_m) = \\sum_{i=1}^m w_i \\mathcal{N}(x; \\mu_i, \\Sigma_i).\n\\end{equation*}\nOne can obtain a sample $x$ from the Gaussian mixture model with a two-step procedure:\n- Sample an index $d$ from the categorical distribution with probabilities $w_1, \\ldots, w_m$.\n- Sample $x$ from $\\mathcal{N}(\\mu_d, \\Sigma_d)$.\n\nWe consider a data set $\\mathbf{X} \\in \\mathbb{R}^{300 \\times 2}$ of 300 samples that are generated from a two-dimensional Gaussian mixture model with mixture weights $w_1 = 0.7$ and $w_2 = 0.3$ and mixture components with parameters\n\\begin{equation*}\n    \\mu_1 = \\begin{bmatrix} 5 \\\\ 7\\end{bmatrix}, \\quad \\Sigma_1 = \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}, \\quad\n    \\mu_2 = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}, \\quad \\Sigma_2 = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}.\n\\end{equation*}"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265}, "id": "YHHYEKlcs_TX", "outputId": "12fc1ae0-dc34-4937-a668-996ff3167fb7", "tags": []}, "outputs": [], "source": "def plot_2dnormal(normal, color):\n    # plot center\n    plt.plot(normal.mean[0], normal.mean[1], '*', ms=5, color=color)\n\n    # get countour grid\n    x, y = np.mgrid[-3:3:.01, -3:3:.01]\n    \n    # rescale x, y\n    x = normal.cov[0,0] * x + normal.mean[0]\n    y = normal.cov[1,1] * y + normal.mean[1]\n    \n    # plot countour\n    pos = np.dstack((x, y)) \n    plt.contour(x, y, normal.pdf(pos), colors=color, levels=3)\n\n# mixture weights\nweights = [0.7, 0.3]\n\n# mixture components\ncomponent1 = multivariate_normal(mean=[1, 2], cov=[[2, 0], [0, 1]])\ncomponent2 = multivariate_normal(mean=[5, 7], cov=[[2, 1], [1, 2]])\ncomponents = [component1, component2]\n\n# sample N = 300 samples from the Gaussian mixture model\nnp.random.seed(1234) # for reproducibility\nN = 300\nX = np.stack([np.random.choice(components, p=weights).rvs() for _ in range(N)])\n\nplt.figure()\nplt.scatter(X[:,0], X[:,1])\nplot_2dnormal(component1, color='red')\nplot_2dnormal(component2, color='blue')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"id": "fluE4c4ZCRx1", "tags": []}, "source": "### (a)\n\nImplement a function `e_step(X, weights, components)` that receives as input\n- data `X` (an array of size $300 \\times 2$ in the model above),\n- mixture `weights` (a list `[p, 1 - p]` in the model above), and\n- mixture `components` (a list of two [`multivariate_normal`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) distributions in the model above).\n\nand outputs a matrix of the likelihood of each data point for each mixture component. More concretely, the entry at the $i$th row and $j$th column of the returned matrix should be\n\\begin{equation*}\n    \\frac{w_i \\mathcal{N}\\left(x_j; \\mu_i, \\Sigma_i\\right)}{\\sum_{k=1}^m w_k \\mathcal{N}\\left(x_j; \\mu_k , \\Sigma_k\\right)}.\n\\end{equation*}\nI.e., in the example here the function should return a atrix of size $2 \\times 300$.\n\nThis computation corresponds to the expectation step in the EM algorithm."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "pamMsGZ3Cdhx", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "Ngfkjp6cBGzM", "tags": []}, "source": "Based on the output of `e_step` we can estimate to which mixture component a data point seems to belong."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 265}, "id": "TlmAnsuLCfBR", "outputId": "5bbaf1ab-d4c6-4a23-fa90-7af12b451e2c", "scrolled": true, "tags": []}, "outputs": [], "source": "# evaluate the function on the dataset\nprobs = e_step(X, weights, components)\n\n# compute to which component data points seem to belong\nbelong_to = probs.argmax(axis=0)\n\n# plot data\nplt.figure()\nplt.scatter(X[:, 0], X[:, 1], c=belong_to)\nfor c in components:\n    plot_2dnormal(c, color='black')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"id": "5a8a_4ny7IwG", "tags": []}, "source": "### (b)\n\nImplement the M-step of the EM algorithm with a function `m_step(X, probs)` that receives as input\n- the data `X` (an array of size $300 \\times 2$ in the model above) and\n- the probabilities `probs` (output of `e_step` implemented above, array of size $300 \\times 2$ in the model above)\n\nand outputs the updated parameter estimates of the Gaussian mixture model, i.e.,\n- the weights of the mixture components (a list `[p, 1 - p]` in the model above) and\n- the mixture components (a list of two [`multivariate_normal`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) distributions in the model above).\n\n*Hint:* The formulas for the updated parameter estimates can be found in eq. 10.16 in http://smlbook.org/."}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "aaqRa9mDFpxP", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"id": "rvqfsNgWCt7N", "tags": []}, "source": "We can perform a sanity check:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "JbLA6_i_GDQP", "outputId": "5918740e-d92c-4e5c-c61c-b8deedb977a7", "tags": []}, "outputs": [], "source": "weights, components = m_step(X, probs)\n\nprint('weights:')\nprint(weights)\nfor i, c in enumerate(components):\n    print('')\n    print('Normal {}'.format(i+1))\n    print('- mean:')\n    print(c.mean)\n    print('- cov:')\n    print(c.cov)"}, {"cell_type": "markdown", "metadata": {"id": "dq-x0rL8GPE5", "tags": []}, "source": "If everything went right, you should obtain estimates close to the ones used to generate the dataset:\n- Mixture weights should be close to $[0.7, 0.3]$\n- The first Gaussian mixture component should be close to\n  \\begin{equation*}\n    \\mathcal{N}\\left(\\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}, \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\right)\n  \\end{equation*}\n- The second Gaussian mixture component should be close to\n  \\begin{equation*}\n    \\mathcal{N}\\left(\\begin{bmatrix} 5 \\\\ 7\\end{bmatrix}, \\begin{bmatrix} 2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\right)\n  \\end{equation*}"}, {"cell_type": "markdown", "metadata": {"id": "5c31XGxiLb1D", "tags": []}, "source": "### (c)\n\nRun the code below to plot 3 iterations of the EM algorithm."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "moydTocLvWbQ", "outputId": "351fa6bb-8556-4ec1-ed7d-12b0ec8a6cd4", "tags": []}, "outputs": [], "source": "# initial parameter estimates\ncomponent1_estimated = multivariate_normal(mean=[0, 0], cov=np.eye(2))\ncomponent2_estimated  = multivariate_normal(mean=[10, 10], cov=np.eye(2))\ncomponents_estimated = [component1_estimated, component2_estimated]\nweights_estimated = [0.5, 0.5]\n\nfor i in range(3):    \n    # expectation step\n    probs = e_step(X, weights_estimated, components_estimated)\n    belong_to = probs.argmax(axis=0)\n    \n    # plot estimates\n    plt.figure()\n    plt.scatter(X[:, 0], X[:, 1], c=belong_to)\n    for c in components_estimated:\n        plot_2dnormal(c, color='black')\n    plt.title(f\"iteration {i}\")\n    plt.show()\n    \n    # maximization step\n    weights_estimated, components_estimated = m_step(X, probs)"}, {"cell_type": "markdown", "metadata": {"id": "gIk96nSiPy1E", "tags": []}, "source": "### (d)\n\nAdapt the code in exercise 11.3 c) and use the EM algorithm to estimate the parameters of a Gaussian mixture model for the two-dimensional encodings of the MNIST training data set computed in exercise 11.1. You have to choose the number of mixture components and the initial parameter estimates."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "mWSxgXA0P2wX", "outputId": "b6fe5546-3288-4a36-a121-7d1e8917c743", "tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "colab": {"collapsed_sections": [], "name": "Session 11.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.19"}}, "nbformat": 4, "nbformat_minor": 1}