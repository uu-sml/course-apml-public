{"cells": [{"cell_type": "code", "execution_count": null, "id": "762abd98", "metadata": {"tags": []}, "outputs": [], "source": "import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn import gaussian_process\nfrom sklearn.gaussian_process import kernels\n\nimport pickle\nfrom urllib.request import urlopen"}, {"cell_type": "markdown", "id": "f7ed677a", "metadata": {"tags": []}, "source": "# Gaussian processes in scikit-learn\n\n## Exercise 5.1: GP posterior\n\nRepeat exercise 4.2 using `GaussianProcessRegressor` from `sklearn.gaussian_process`, only plotting the credibility regions based on the posterior predictive variance (not drawing samples).\n\nSome useful hints:\n- As all supervised machine learning methods in `sklearn`, you first have to construct an object from the model class (in this case `GaussianProcessRegressor`), and thereafter train it on data by using its member function `fit()`. To obtain predictions, use the member function `predict()`. To the latter, you will either have to pass `return_std=True` or `return_cov=True` in order to obtain information about the posterior predictive variance.\n- When you construct the model, you have to define a kernel. The kernels are available in `sklearn.gaussian_process.kernel`, where the squared exponential/RBF kernel is available as `RBF`.\n- The function `fit()` automatically optimizes the hyperparameters. To turn that feature off, you have to pass the argument `optimizer=None` to `GaussianProcessRegressor`.\n- To include the measurement noise, you can formulate it as part of the kernel by using the kernel `WhiteKernel`."}, {"cell_type": "code", "execution_count": null, "id": "5627f29c", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "fdc3bb36", "metadata": {"tags": []}, "source": "## Exercise 5.2: Learning hyperparameters\n\nUntil now, we have made GP regression using predefined hyperparameters, such as the lengthscale $\\ell$ and noise variance $\\sigma^2$. In this exercise, we will estimate $\\ell$ and $\\sigma^2$ from the data by maximizing the marginal likelihood (cf. exercise 4.4 and 4.5). That is done automatically\nby the `fit()` function in scikit-learn. Use, as before, the RBF kernel and measurement noise together, this time with the data\n\\begin{equation*}\n\\mathbf{x} = \\begin{bmatrix}-5 &-3 &0 &0.1 &1 &4.9 &5\\end{bmatrix}^\\mathsf{T} \\,\\text{ and }\\,\n\\mathbf{y} = \\begin{bmatrix}0 &-0.5 &1 &0.7 &0 &1 &0.7\\end{bmatrix}^\\mathsf{T}.\n\\end{equation*}"}, {"cell_type": "markdown", "id": "37ec3592", "metadata": {"tags": []}, "source": "### (a)\n\nYou still have to provide an initial value of the hyperparameters. Try $\\ell = 1$ and $\\sigma^2 = 0.1$. What hyperparameters do you get when optimizing? Plot the corresponding mean and credibility regions."}, {"cell_type": "code", "execution_count": null, "id": "e5bd047e", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "66056357", "metadata": {"tags": []}, "source": "### (b)\n\nTry instead to initialize with $\\ell = 10$ and $\\sigma^2 = 1$. What do you get now?"}, {"cell_type": "code", "execution_count": null, "id": "413686ba", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "e165cbe7", "metadata": {"tags": []}, "source": "### (c)\n\nTry to explain what happens by making a grid over different hyperparameter values, and inspect the marginal likelihood for each point in that grid. The `GaussianProcessRegressor` class has a member function `log_marginal_likelihood()` which you may use. (Do not forget to turn off the hyperparameter optimization!)"}, {"cell_type": "code", "execution_count": null, "id": "77827e38", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "ff3a836b", "metadata": {"tags": []}, "source": "## Exercise 5.3: Modeling CO\u2082 levels\n\nThe amount of carbon dioxide in the atmosphere has been measured continuously at the Mauna Loa observatory, Hawaii. In this problem, you should use a Gaussian process to model the data from 1958 to 2003, and see how well that model can be used for predicting the data from 2004-2019. They present their latest data at [their homepage](https://www.esrl.noaa.gov/gmd/ccgg/trends/), but for your convenience you can use the data in the format available [here](https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa). You can load the data with the following code snippet:"}, {"cell_type": "code", "execution_count": null, "id": "65197ba6", "metadata": {"tags": []}, "outputs": [], "source": "# download data\ndata = pickle.load(\n    urlopen(\"https://github.com/gpschool/labs/raw/2019/.resources/mauna_loa\")\n)\n\n# extract observations and test data\nx = data['X'].flatten()\ny = data['Y'].flatten()\nxtest = data['Xtest'].flatten()\nytest = data['Ytest'].flatten()"}, {"cell_type": "markdown", "id": "f474d2be", "metadata": {"tags": []}, "source": "Here, `x` and `y` contain your training data.\n\nStart exploring some simple kernels, and thereafter you may have a look at page 118-122 of the book [Gaussian processes for machine learning](http://www.gaussianprocess.org/gpml/chapters/RW5.pdf) for some inspiration on how to design a more bespoke kernel for this problem."}, {"cell_type": "code", "execution_count": null, "id": "7a347396", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "kernelspec": {"display_name": "exercises-apml", "language": "python", "name": "exercises-apml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 5}