{"cells": [{"cell_type": "code", "execution_count": null, "id": "618bcfd4", "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nfrom sklearn import gaussian_process\nfrom sklearn.gaussian_process import kernels"}, {"cell_type": "markdown", "id": "eb240ca9", "metadata": {"tags": []}, "source": "# Gaussian processes: Session 1\n\n## Exercise 8.1: Sample from GP prior\n\nIn this exercises we will write the code needed to draw and plot samples of $f$ from a Gaussian process prior with squared exponential (or, equivalently, RBF) kernel\n\n$$\nf \\sim \\mathcal{GP}(m, \\kappa), \\qquad m(x) = 0 \\text{ and } \\kappa(x, x') = \\sigma^2_f \\exp{\\Big(\u2212\\frac{1}{2l^2} {\\|x - x'\\|}^2\\Big)}.\n$$\n\nTo implement this, we choose a vector of $m$ test input points $\\mathbf{x}_*$.\nWe will choose $\\mathbf{x}_*$ to contain sufficiently many points, such that it will *appear* as a continuous line on the screen.\nWe then evaluate the $m \\times m$ covariance matrix $\\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)$ and thereafter generate samples from the multivariate normal distribution\n\n$$\nf(\\mathbf{x}_\u2217) \\sim \\mathcal{N}\\big(m(\\mathbf{x}_\u2217), \\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)\\big).\n$$"}, {"cell_type": "markdown", "id": "0013d607", "metadata": {"tags": []}, "source": "### (a)\n\nUse `np.linspace` to construct a vector $\\mathbf{x}_*$ with $m = 101$ elements equally spaced from -5 to 5."}, {"cell_type": "code", "execution_count": null, "id": "b1051b9a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "1ec0520b", "metadata": {"tags": []}, "source": "### (b)\n\nConstruct a mean vector $m(\\mathbf{x}_\u2217)$ with 101 elements all equal to zero and the $101 \\times 101$ covariance matrix $\\kappa(\\mathbf{x}_*, \\mathbf{x}_\u2217)$.\nThe expression for $\\kappa(\\cdot, \\cdot)$ is given above.\nLet the hyperparameters be $\\ell^2 = 2$ and $\\sigma^2_f = 1$."}, {"cell_type": "code", "execution_count": null, "id": "87147e52", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "efda7b49", "metadata": {"tags": []}, "source": "### (c)\n\nUse `stats.multivariate_normal` from `scipy` (you might need to use the option `allow_singular=True`) to draw 25 samples $f^{(1)}(\\mathbf{x}_*), \\ldots, f^{(25)}(\\mathbf{x}_*)$ from the multivariate normal distribution $f(\\mathbf{x}_\u2217) \\sim \\mathcal{N}\\big(m(\\mathbf{x}_*), \\kappa(\\mathbf{x}_*, \\mathbf{x}_*)\\big)$."}, {"cell_type": "code", "execution_count": null, "id": "8860de51", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "7862dce0", "metadata": {"tags": []}, "source": "### (d)\n\nPlot the samples $f^{(1)}(\\mathbf{x}_\u2217), \\ldots, f^{(25)}(\\mathbf{x}_\u2217)$ versus the input vector $\\mathbf{x}_*$."}, {"cell_type": "code", "execution_count": null, "id": "af7c64e7", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "e59f6757", "metadata": {"tags": []}, "source": "### (e)\n\nTry another value of $\\ell$ and repeat (b)-(d). How do the two plots differ, and why?"}, {"cell_type": "code", "execution_count": null, "id": "d76df64f", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "3a807698", "metadata": {"tags": []}, "source": "## Exercise 8.2: GP posterior I\n\nIn this exercise we will perform Gaussian process regression.\nThat means, based on the $n$ observations $\\mathcal{D} = \\{x_i, f(x_i)\\}^n_{i=1}$ and the prior belief $f \\sim \\mathcal{GP}\\big(0, \\kappa(x, x')\\big)$, we want to find the posterior $p(f | \\mathcal{D})$.\n(In the previous problem, we were only concerned with the prior $p(f)$, not conditioned on having observed the data $\\mathcal{D}$.)\nWe consider the same Gaussian process prior (same mean $m(x)$ and $\\kappa(x, x')$ and hyperparameters) as in the previous exercise."}, {"cell_type": "markdown", "id": "f0d751d1", "metadata": {"tags": []}, "source": "### (a)\n\nConstruct two vectors $\\mathbf{x} = [-4, -3, -1, 0, 2]^\\mathsf{T}$ and $\\mathbf{y} = [-2, 0, 1, 2, -1]^\\mathsf{T}$, which will be our training data (that is, $n = 5$)."}, {"cell_type": "code", "execution_count": null, "id": "a458b7ea", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "8631bc66", "metadata": {"tags": []}, "source": "### (b)\n\nKeep $\\mathbf{x}_\u2217$ as in the previous problem. In addition to the $m \\times m$ matrix $\\kappa(\\mathbf{x}_\u2217, \\mathbf{x}_\u2217)$, now also compute the $n \\times m$ matrix $\\kappa(\\mathbf{x}, \\mathbf{x}_\u2217)$ and the $n \\times n$ matrix $\\kappa(\\mathbf{x}, \\mathbf{x})$.\n\n*Hint:* You might find it useful to define a function that returns $\\kappa(x, x')$, taking $x$ and $x'$ as arguments."}, {"cell_type": "code", "execution_count": null, "id": "7b7b1b8a", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "87207f59", "metadata": {"tags": []}, "source": "### (c)\n\nUse the training data $\\mathbf{x}$, $\\mathbf{y}$ and the matrices constructed in (b) to compute the posterior mean $\\boldsymbol{\\mu}_{\\mathrm{posterior}}$ and the posterior covariance $\\mathbf{K}_{\\mathrm{posterior}}$ for $\\mathbf{x}_\u2217$, by using the equations for conditional multivariate normal distribution."}, {"cell_type": "code", "execution_count": null, "id": "e2b12671", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "cce3cf6c", "metadata": {"tags": []}, "source": "### (d)\n\nIn a similar manner as in (c) and (d) in the previous problem, draw 25 samples from the multivariate distribution $f(\\mathbf{x}_\u2217) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\mathrm{posterior}}, \\mathbf{K}_{\\mathrm{posterior}})$ and plot these samples ($f^{(j)} (\\mathbf{x}_\u2217)$ vs. $\\mathbf{x}_\u2217$) together with the posterior mean ($\\boldsymbol{\\mu}_{\\mathrm{posterior}}$ vs. $\\mathbf{x}_\u2217$) and the actual measurements ($\\mathbf{f}$ vs. $\\mathbf{x}$).\nHow do the samples in this plot differ from the prior samples in the previous problem?"}, {"cell_type": "code", "execution_count": null, "id": "1f265ced", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "67d60cf1", "metadata": {"tags": []}, "source": "### (e)\n\nInstead of plotting samples, plot a credibility region.\nHere, a credibility region is based on the (marginal) posterior variance.\nThe 68% credibility region, for example, is the area between $\\boldsymbol{\\mu}_{\\mathrm{posterior}} - \\sqrt{\\mathbf{K}^d_{\\mathrm{posterior}}}$ and $\\boldsymbol{\\mu}_{\\mathrm{posterior}} + \\sqrt{\\mathbf{K}^d_{\\mathrm{posterior}}}$, where $\\mathbf{K}^d_{\\mathrm{posterior}}$ is a vector with the diagonal elements of $\\mathbf{K}_{\\mathrm{posterior}}$.\nWhat is the connection between the credibility regions and the samples you drew previously?"}, {"cell_type": "code", "execution_count": null, "id": "6d781fb9", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "d5c4d698", "metadata": {"tags": []}, "source": "### (f)\n\nNow, consider the setting where the measurements are corrupted with noise, $y_i = f(\\mathbf{x}_i) + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\nUse $\\sigma^2 = 0.1$ and repeat (c)-(e) with this modification of the model.\nWhat is the difference in comparison to the previous plot?\nWhat is the interpretation?"}, {"cell_type": "code", "execution_count": null, "id": "bd40020b", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "39ff1cdb", "metadata": {"tags": []}, "source": "### (g)\n\nExplore what happens with another length scale $\\ell$."}, {"cell_type": "code", "execution_count": null, "id": "a1b44903", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "50548d5d", "metadata": {"tags": []}, "source": "## Exercise 8.3: Other covariance functions/kernels\n\nThe squared exponential kernel/covariance function gives samples which are smooth and infinitely continuously differentiable.\nOther kernels make other assumptions.\nNow try the previous problems using the exponential kernel,\n\n$$\n\\kappa(x, x') = \\sigma^2_f \\exp{\\bigg(-\\frac{1}{l} \\|x - x' \\|\\bigg)},\n$$\n\nand the Mat\u00e9rn kernel with parameter $\\nu = 3/2$,\n\n$$\n\\kappa(x, x') = \\sigma^2_f \\bigg(1 + \\frac{\\sqrt{3}}{l} \\|x - x' \\|\\bigg) \\exp{\\bigg(-\\frac{\\sqrt{3}}{l} \\|x - x' \\|\\bigg)}.\n$$"}, {"cell_type": "code", "execution_count": null, "id": "da81bc1d", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "272aebea", "metadata": {"tags": []}, "source": "## Exercise 8.4: GP posterior II\n\nRepeat Exercise 8.2 using `GaussianProcessRegressor` from `sklearn.gaussian_process`, only plotting the credibility regions based on the posterior predictive variance (not drawing samples).\n\nSome useful hints:\n\n- As all supervised machine learning methods in `sklearn`, you first have to construct an object from the model class (in this case `GaussianProcessRegressor`), and thereafter train it on data by using its member function `fit()`.\n  To obtain predictions, use the member function `predict()`.\n  To the latter, you will either have to pass `return_std=True` or `return_cov=True` in order to obtain information about the posterior predictive variance.\n- When you construct the model, you have to define a kernel.\n  The kernels are available in `sklearn.gaussian_process.kernel`, where the squared exponential/RBF kernel is available as `RBF`.\n- The function `fit()` automatically optimizes the hyperparameters.\n  To turn that feature off, you have to pass the argument `optimizer=None` to `GaussianProcessRegressor`.\n- To include the measurement noise, you can formulate it as part of the kernel by using the kernel `WhiteKernel`."}, {"cell_type": "code", "execution_count": null, "id": "5627f29c", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.6"}, "vscode": {"interpreter": {"hash": "e0466e4bbc1f6f77653f92f7ee99fe375173484495b8b5339e7493ccb72bc580"}}}, "nbformat": 4, "nbformat_minor": 5}