{"cells": [{"cell_type": "code", "execution_count": null, "id": "73ff33ef", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "abe73aa6", "metadata": {"tags": []}, "source": "# Semi-supervised and generative models\n\nIn the lectures the Variational Autoencoder (VAE) was introduced as a specific generative latent variable model. Compared to probabilistic PCA, VAE employs nonlinear functions (neural network) for encoding and decoding, and its learning is achieved by maximizing the evidence lower bound (ELBO). \n\n\n### Exercise 9.1 - Understand and derive the ELBO\n\nGiven observed data points $\\{ x_n \\}_{n=1}^N$ distributed according to some unknown data distribution $p_\\mathrm{data}(x)$, the goal of generative models is to learn a model $p_\\theta(x)$ that we can sample from, such that $p_\\theta(x)$ is as similar to $p_\\mathrm{data}(x)$ as possible. \n\nLet's consider the following latent variable model:\n\n$$\nx \\sim p_\\theta(x|z), \\quad z \\sim p(z).\n$$\n\nwhere $p(z)$ could be a simple Gaussian and $\\theta$ is the parameter of a neural network. Then the marginal likelihood of $x$ can be writen as\n\n$$\np_\\theta(x) = \\int p_\\theta(x | z) p(z) dz,\n$$\n\nwhich is the objective we want to maximize. But unfortunately, this integral is usually intractable due to the nonlinar and high-dimensional functions (i.e., neural network) inside $z$! Instead, we try to find a lower bound on $p_\\theta(x)$ that is easier to compute, which gives the ELBO objective.\n\nSpecifically, we focus on maximizing the log-likelihood $\\log p_\\theta(x)$ by introducing an approximate posterior $q_\\phi(z|x)$, as\n\n$$\n\\log p_\\theta(x) = \\log \\int p_\\theta(x|z) p(z) \\, dz = \\log \\int q_\\phi(z|x)\\,\\frac{p_\\theta(x|z) p(z)}{q_\\phi(z|x)} dz = \\log \\mathbb{E}_{q_\\phi(z|x)}\\left[\\frac{p_\\theta(x|z) p(z)}{q_\\phi(z|x)}\\right].\n$$\n\nApplying [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality) to this equation results in the **E**vidence **L**ower **BO**und (ELBO):\n\n$$\n\\begin{aligned}\n\\mathcal{L}(x; \\theta, \\phi) &= \\log \\mathbb{E}_{q_\\phi(z|x)}\\left[\\frac{p_\\theta(x|z) p(z)}{q_\\phi(z|x)}\\right] \\\\ \n&\\geq \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log \\frac{p_\\theta(x|z) p(z)}{q_\\phi(z|x)}\\right] \\\\ \n&= \\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right] - D_{KL}(q_\\phi(z|x) \\| p(z)).\n\\end{aligned}\n$$\n\nThe first term $\\mathbb{E}_{q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right]$ is called the reconstruction loss maximizing the log-likelihood and the second term $-D_{KL}(q_\\phi(z|x) \\| p(z))$ is a regularizer minimizing the distance between $q_\\phi(z|x)$ and $p(z)$.\n\nSo, optimizing the ELBO means learning an encoder ($\\phi$) and a decoder ($\\theta$) that fits the dataset while ensuring the encoded latent variable $z$ remains close to the prior $p(z)$."}, {"cell_type": "markdown", "id": "884a414e", "metadata": {"tags": []}, "source": "### Exercise 9.2 - Implement the VAE\n\nIn this exercise, you will learn the key techniques that make VAEs actually trainable and will implement the VAE on a small dataset.\n\n#### Compute the KL divergence\n\nNote that the KL term has a closed-form if both the approximate posterior $q_\\phi(z|x)$ and prior $p(z)$ are Gaussian distribution. For example, we define $p(z)$ a *standard multivariate Gaussian* and $q_\\phi(z|x)$ a multivariate Gaussian with a diagonal covariance matrix:\n\n$$\np(z) = \\mathcal{N}(0, I), \\quad q_\\phi(z|x) = \\mathcal{N}(\\mu_\\phi(x), \\mathrm{diag}(\\sigma^2_\\phi(x))).\n$$\n\nThe [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Examples) between them has a simple yet analytical solution given by\n\n$$\nD_{KL}(q_\\phi(z|x) \\| p(z)) = -\\frac{1}{2} \\sum_{n=1}^N (1 + \\log (\\sigma_\\phi^2(x_n)) - \\mu_\\phi^2(x_n) - \\sigma_\\phi^2(x_n)),\n$$\n\nwhere $N$ is the number of dimensions in the latent space. $\\mu_\\phi$ and $\\sigma_\\phi^2$ are the mean and variance generated by the encoder, respectively.\n\n**(a) KL Implementation:**"}, {"cell_type": "code", "execution_count": null, "id": "8419a9a9", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "7f305d45", "metadata": {"tags": []}, "source": "#### Reparameterization trick \n\nThe question now turns to how to compute the expectation over posterior i.e., $\\mathbb{E}_{z \\sim q_\\phi(z|x)}\\left[\\log p_\\theta(x|z)\\right]$). However, the latent variable $z$ is sampled from $q_\\phi(z | x)$, whose parameter $\\phi$ is what we want to optimize as well. And the sampling process itself, such as , is not differentiable. Then how can we compute the gradient $\\nabla_\\phi$ from the random sampling node and make sure it is avaiable in backpropagation? The solution is the *Reparameterization Trick*.\n\nThe trick is to separate the randomness from the parameters. Instead of directly sampling $z \\sim q_\\phi(z | x)$, we generate $z$ from the following two steps:\n\n1. Sample a random noise from the standard normal distribution: $\\epsilon \\sim \\mathcal{N}(0, I)$.\n2. Obtain $z$ from a deterministic, differentiable function: $z = \\mu_\\phi(x) + \\sigma_\\phi(x) \\cdot \\epsilon$.\n\nIn this way, $z$ is still a random variable with distribution $\\mathcal{N}(\\mu_\\phi(x), \\sigma^2_\\phi(x))$, but the randomness comes entirely from $\\epsilon$ rather than the encoder.\n\n**(b) Sampling Implementation:**"}, {"cell_type": "code", "execution_count": null, "id": "2f565053", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "2bdba11d", "metadata": {"tags": []}, "source": "**(c) The final ELBO loss function:**"}, {"cell_type": "code", "execution_count": null, "id": "54827d21", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "f7781210", "metadata": {"tags": []}, "source": "Now we can define the Network architecture and train the VAE model.\n\n**Task:** Read through and understand the implementation of the training procedure."}, {"cell_type": "code", "execution_count": null, "id": "abc3107f", "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "2a03901b", "metadata": {"tags": []}, "source": "### Exercise 9.3 - Model Inference\n\nOnce trained, we can obtain new data points by sampling from the noise distribution."}, {"cell_type": "code", "execution_count": null, "id": "d6bd6763", "metadata": {"tags": []}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.19"}}, "nbformat": 4, "nbformat_minor": 5}