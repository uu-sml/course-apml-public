{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ROfdN0x2XNPW", "tags": []}, "outputs": [], "source": "%matplotlib inline\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import optim\nfrom torchvision import datasets, transforms\n\n# use GPU for computation if possible\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# download and extract the MNIST data set\ntrainset = datasets.MNIST(root='./data', train=True, download=True,\n                          transform=transforms.ToTensor())\ntestset = datasets.MNIST(root='./data', train=False, download=True,\n                         transform=transforms.ToTensor())\n\n# extract a complete PyTorch dataset\ndef extract(dataset):\n    datasize = len(dataset)\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=datasize, shuffle=False)\n    return next(iter(dataloader))\n\n# extract all training and test images and labels into PyTorch tensors\ntrain_images, train_labels = extract(trainset)\ntest_images, test_labels = extract(testset)\n\n# flatten training and test images to vectors with 28 x 28 = 784 entries\n# and move to the GPU if available\ntrain_images = train_images.view(-1, 784).to(device)\ntest_images = test_images.view(-1, 784).to(device)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ZXGqSi1BbMzX", "tags": []}, "source": "# Probabilistic PCA\n\nIn this notebook we will train and analyze a probabilistic PCA model on the MNIST data set. Our goal is to perform dimensionality reduction: we reduce the high-dimensional images to a two-dimensional representation that still captures some of the important aspects of the images. We will analyze how well images can be reconstructed from the lower dimensional representations and try to generate images that look similar to the images in the MNIST data set."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Before you start\n\nIn exercise 8.1 we performed a (non-probabilistic!) PCA on the MNIST data. It is strongly recommended to solve exercise 8.1 before working through this notebook. You can download the notebook for exercise 8.1 [here](https://uu-sml.github.io/course-apml-public/exercises/Session11.ipynb) and run it on your computer, or you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-apml-public/blob/master/exercises/Session11.ipynb).\n\nIn this notebook we use [PyTorch](https://pytorch.org/), an open source software library for machine learning. Make sure that you have installed the latest version of PyTorch if you run the notebook on your computer. Alternatively, you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-apml-public/blob/master/lab/PPCA.ipynb), which also allows you to use GPUs which might speed up some computations.\n\nA Jupyter notebook with an introduction to PyTorch can be downloaded from [here](https://uu-sml.github.io/course-sml-public/lab/introduction.ipynb). Alternatively, you can [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-sml-public/blob/master/lab/introduction.ipynb). Reading and running the notebook is highly recommended, since it introduces important concepts and commands that are required in this notebook."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ZXGqSi1BbMzX", "tags": []}, "source": "## Theoretical background\n\nLet $\\mathbf{x} \\in \\mathbb{R}^{784}$ represent a $28 \\times 28$-pixel grayscale image and $\\mathbf{z} \\in \\mathbb{R}^M$ be an $M$-dimensional latent variable with $M < 784$.\n\nInstead of regular PCA we now use [probabilistic PCA](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196) as a model for dimensionality reduction and feature extraction, since probabilistic PCA can be altered and extended quite easily and allows a probabilistic interpretation of the encodings, which helps us with generating new MNIST-like images. In our setting, the probabilistic PCA model is given by \n\\begin{align*}\n  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_{784}\\right), \\\\\n  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_M),\n\\end{align*}\nwith parameters $\\mathbf{W} \\in \\mathbb{R}^{784 \\times M}$, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{784}$, and $\\sigma^2 > 0$.\n\nIn the preparatory exercises, you showed that for the probabilistic PCA model\n\\begin{equation*}\n    p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}, \\mathbf{C}),\n\\end{equation*}\nwhere\n\\begin{equation*}\n    \\mathbf{C} = \\mathbf{W} \\mathbf{W}^\\intercal + \\sigma^2 \\mathbf{I}_{784}.\n\\end{equation*}\nThus the log-likelihood of i.i.d. data samples $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$ is given by\n\\begin{equation*}\n    \\log p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = - \\frac{N}{2}(784 \\log{(2\\pi)} + \\log{|\\mathbf{C}|}) - \\frac{1}{2} \\sum_{n=1}^N {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\intercal {\\mathbf{C}}^{-1} {(\\mathbf{x}_n - \\boldsymbol{\\mu})}.\n\\end{equation*}"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "ZXGqSi1BbMzX", "tags": []}, "source": "## Cost function\n\n[Tipping and Bishop](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196) actually derived an expression for the maximum likelihood solution of $\\mathbf{W}$, $\\boldsymbol{\\mu}$, and $\\sigma^2$. However, in this lab we do not try to obtain the maximum likelihood solution with the algorithms suggested in their original work. Instead, since the log-likelihood is differentiable with respect to the parameters, we will optimize the parameters with gradient descent, similar to the linear regression example in the Jupyter notebook [Introduction to PyTorch](https://uu-sml.github.io/course-sml-public/lab/introduction.ipynb) (you can also [open it on Google Colab](https://colab.research.google.com/github/uu-sml/course-sml-public/blob/master/lab/introduction.ipynb)).\n\nWe try to minimize the cost function\n\\begin{equation*}\n  J(\\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = \\log{|\\mathbf{C}|} + \\frac{1}{N} \\sum_{n=1}^N {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\mathsf{T} {\\mathbf{C}}^{-1} {(\\mathbf{x}_n - \\boldsymbol{\\mu})},\n\\end{equation*}\nwhere we neglected all additive constant terms of the log-likelihood and scaled it by $N / 2$.\n\nIt is computationally much cheaper to work with the\nmatrix\n\\begin{equation*}\n\\mathbf{M} = \\mathbf{W}^\\mathsf{T} \\mathbf{W} + \\sigma^2 \\mathbf{I}_M \\in \\mathbb{R}^{M \\times M}\n\\end{equation*}\nthan with the matrix $\\mathbf{C} \\in \\mathbb{R}^{784 \\times 784}$.\nActually one can exploit the special structure of the matrix\n$\\mathbf{C}$ and use the [matrix determinant lemma](https://en.wikipedia.org/wiki/Matrix_determinant_lemma#Generalization) and the [Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity) to show that\n\\begin{equation*}\n      J(\\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = (784 - M) \\log \\sigma^2 +\n      \\log{|\\mathbf{M}|} +\n      \\frac{1}{N\\sigma^2} \\sum_{n=1}^N \\left(\\|\\mathbf{x}_n - \\boldsymbol{\\mu}\\|_2^2 -\n        {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\mathsf{T} \\mathbf{W}\n        {\\mathbf{M}}^{-1} \\mathbf{W}^\\mathsf{T}\n        (\\mathbf{x}_n - \\boldsymbol{\\mu}) \\right).\n\\end{equation*}\n\nHere we consider $M = 2$, i.e., $\\mathbf{z}$ is two-dimensional. We start by implementing the cost function."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "72Dm5wSH_USn", "tags": []}, "source": "### Task\n\nThe following Python function `cost_function` will be used to evaluate the cost function $J$ for a given data set\n\\begin{equation*}\n\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^\\intercal \\\\ \\vdots \\\\ \\mathbf{x}_N^\\intercal \\end{bmatrix}\n\\end{equation*}\nand parameters $\\mathbf{W}$, $\\boldsymbol{\\mu}$, and $\\log \\sigma^2$. To enforce $\\sigma^2 > 0$, we optimize the real-valued parameter $\\log \\sigma^2$ instead of $\\sigma^2$. Read through and try to understand the existing implementation. Add a final line to the function that computes and returns the value of the cost function by making use of the already computed values and matrices.\n\n*Hint*: Use the PyTorch functions [`torch.pow`](https://pytorch.org/docs/stable/torch.html#torch.pow) for taking the power of each element in a PyTorch tensor, and [`torch.sum`](https://pytorch.org/docs/stable/torch.html#torch.sum) and [`torch.mean`](https://pytorch.org/docs/stable/torch.html#torch.mean) for computing the sum and the mean of a PyTorch tensor (it is possible to specify the dimension along which the operation should be performed)."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "HGISRv6T0fiH", "tags": []}, "outputs": [], "source": "def cost_function(X, W, mu, logsigma2):\n    # flatten the data into a matrix with 28 x 28 = 784 columns\n    X = X.view(-1, 784)\n\n    # compute matrix Xshifted with rows (x_n - mu)^T\n    # note: mu is defined as a row vector\n    Xshifted = X - mu\n    \n    # compute matrix Y with rows (x_n - mu)^T * W\n    Y = Xshifted.mm(W)\n\n    # compute matrix M = W^T * W + sigma^2 I\n    sigma2 = logsigma2.exp()\n    M = W.t().mm(W) + torch.diagflat(sigma2.expand(2))\n\n    # compute the log-determinant of M\n    Mlogdet = M.logdet()\n\n    # compute the inverse of M\n    Minverse = M.inverse()\n\n    # compute vector v with v[n] = (x_n - mu)^T * W * M^(-1) * W^T * (x_n - mu)\n    v = Y.mm(Minverse).mm(Y.t()).diagonal()\n    \n    # put everything together and compute loss\n"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "#### Note\n\nIn particular for higher-dimensional latent spaces one would want to use the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\mathbf{M}$ to speed up the computation of $\\log{|\\mathbf{M}|}$ and ${(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\mathsf{T} \\mathbf{W} {\\mathbf{M}}^{-1} \\mathbf{W}^\\mathsf{T} (\\mathbf{x}_n - \\boldsymbol{\\mu})$ (actually, that is exactly how it is implemented in [PyTorch](https://pytorch.org/docs/stable/_modules/torch/distributions/lowrank_multivariate_normal.html))."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Training\n\nWe train the model with [Adam](https://arxiv.org/abs/1412.6980), a gradient-based optimization algorithm with adaptive learning rates for different parameters. Instead of evaluating the gradient based on all images in the training data set\nin every step, we compute it from a randomly chosen subset of the training data, a so-called minibatch. The main idea is that the gradients computed from a large enough random subset of the data should be roughly similar to the gradient evaluated on the whole data set, but by using a subset of the data set the computation time can be improved.\n\nIn PyTorch data loaders are used for iterating through minibatches. The following code snippet creates data loaders of the MNIST training and test data sets that return minibatches of 500 images and their corresponding labels upon iteration. The samples in the training data set are shuffled whereas the samples in the test data set are always returned in the same order."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "hWPurwTXf3Nv", "tags": []}, "outputs": [], "source": "# define data loaders\n# `pin_memory=True` is helpful when working with GPUs: https://pytorch.org/docs/stable/data.html#memory-pinning\ntrain_data = torch.utils.data.DataLoader(\n    trainset, batch_size=500, shuffle=True, pin_memory=device.type=='cuda'\n)\ntest_data = torch.utils.data.DataLoader(\n    testset, batch_size=500, pin_memory=device.type=='cuda'\n)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "DesCP5q9gJmc", "tags": []}, "source": "The next code snippet shows a complete implementation of the training procedure."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 630}, "colab_type": "code", "id": "Cry2BVXTgXv-", "outputId": "388d2e83-43ab-4f45-d972-b867b13de0d4", "tags": []}, "outputs": [], "source": "# define the initial model parameters\n# we use the GPU if available (`device=device`)\n# and track gradient information (`requires_grad=True`)\nW = torch.randn((784, 2), device=device, requires_grad=True)\nmu = torch.zeros(1, 784, device=device, requires_grad=True)\nlogsigma2 = torch.zeros(1, device=device, requires_grad=True)\n\n# define the optimizer\noptimizer = optim.Adam([W, mu, logsigma2], lr=0.01)\n\n# track the training and test loss\ntraining_loss = []\ntest_loss = []\n\n# optimize parameters for 20 epochs\nfor i in range(20):\n    # for each minibatch\n    for batch_X, _ in train_data:\n        # move batch to the GPU if available\n        batch_X = batch_X.to(device)\n\n        # reset the gradient information\n        optimizer.zero_grad()\n\n        # evaluate the cost function on the training data set\n        loss = cost_function(batch_X, W, mu, logsigma2)\n\n        # update the statistics\n        training_loss.append(loss.item())\n        test_loss.append(float('nan'))\n\n        # perform backpropagation\n        loss.backward()\n\n        # perform a gradient descent step\n        optimizer.step()\n\n    # evaluate the model after every epoch\n    with torch.no_grad():\n        # evaluate the cost function on the test data set\n        accumulated_loss = 0\n        for batch_X, _ in test_data:\n            # move batch to the GPU if available\n            batch_X = batch_X.to(device)\n\n            # compute loss\n            loss = cost_function(batch_X, W, mu, logsigma2)\n            accumulated_loss += loss.item()\n\n        # update the statistics\n        test_loss[-1] = accumulated_loss / len(test_data)\n            \n    print(f\"Epoch {i + 1:2d}: training loss {training_loss[-1]: 9.3f}, \"\n          f\"test loss {test_loss[-1]: 9.3f}\")\n        \n# plot the tracked statistics\nplt.figure()\niterations = np.arange(1, len(training_loss) + 1)\nplt.scatter(iterations, training_loss, label='training loss')\nplt.scatter(iterations, test_loss, label='test loss')\nplt.legend()\nplt.xlabel('iteration')\nplt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nRead through and try to understand the implementation of the training procedure above. Answer the following questions:\n\n- How does it differ from the implementation of the training procedure in the linear regression example?\n- How were the parameters initialized?\n- What learning rate did we use in the gradient descent algorithm?\n- For how many iterations was the model trained?"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "Since we do not update the parameters anymore, no gradients have to be computed in the following sections. We can prevent PyTorch from tracking all our computations and building computational graphs by changing the attribute `requires_grad`."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "W.requires_grad = False\nmu.requires_grad = False\nlogsigma2.requires_grad = False"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JWueTFkfMVvt", "tags": []}, "source": "## Encodings \n\nIn the preparatory exercises we showed that the distribution of the latent representation $\\mathbf{z}$ conditioned on an image $\\mathbf{x}$ is also Gaussian and given by\n\\begin{equation*}\n    p(\\mathbf{z} \\,|\\, \\mathbf{x}) = \\mathcal{N}\\left(\\mathbf{z}; \\mathbf{M}^{-1} \\mathbf{W}^\\mathsf{T} (\\mathbf{x} - \\boldsymbol{\\mu}), \\sigma^2 \\mathbf{M}^{-1}\\right),\n\\end{equation*}\nwhere\n\\begin{equation*}\n\\mathbf{M} = \\mathbf{W}^\\mathsf{T} \\mathbf{W} + \\sigma^2 \\mathbf{I}_2.\n\\end{equation*}\nWe can use this result to encode the MNIST images in the lower-dimensional latent space. In contrast to regular PCA there exists not one unique representation of an image in the latent space, but instead each image gives rise to a whole distribution of representations in the latent space. Here we use the mean\n\\begin{equation*}\n\\mathbf{M}^{-1} \\mathbf{W}^\\mathsf{T} (\\mathbf{x} - \\boldsymbol{\\mu})\n\\end{equation*}\nof the normal distribution as encoding. Alternatively, one could sample from the normal distribution."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "DoJ2RGL6Lz0x", "tags": []}, "outputs": [], "source": "# compute M = W^T * W + sigma^2 * I\nM = W.t().mm(W) + torch.diagflat(logsigma2.exp().expand(2))\n\n# compute the inverse of M\nMinv = torch.inverse(M)\n\n# compute encodings of the training images\n# use the GPU if available\ntrain_encodings = (train_images - mu).mm(W).mm(Minv)\n\n# compute encodings of the test images\n# use the GPU if available\ntest_encodings = (test_images - mu).mm(W).mm(Minv)"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "We generate 2D scatter plots of the encodings that show the clusters for different labels in the label space. We make use of the function `plot_encodings` below."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# plot `train_encodings` and `test_encodings` with colorcoding of the corresponding labels\ndef plot_encodings(train_encodings, train_labels, test_encodings, test_labels):\n    # create two plots side by side\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n\n    # plot encodings of training data\n    ax = axes[0]\n    ax.scatter(\n        train_encodings[:, 0].cpu().numpy(), train_encodings[:, 1].cpu().numpy(),\n        c=train_labels, cmap=plt.cm.tab10, vmin=-0.5, vmax=9.5, alpha=0.7\n    )\n    ax.set_xlabel(\"$z_1$\")\n    ax.set_ylabel(\"$z_2$\")\n    ax.set_title(\"training data\")\n    \n    # plot encodings of test data\n    ax = axes[1]\n    scatter = ax.scatter(\n        test_encodings[:, 0].cpu().numpy(), test_encodings[:, 1].cpu().numpy(),\n        c=test_labels, cmap=plt.cm.tab10, vmin=-0.5, vmax=9.5, alpha=0.7\n    )\n    ax.set_xlabel(\"$z_1$\")\n    ax.set_ylabel(\"$z_2$\")\n    ax.set_title(\"test data\")\n\n    # add colorbar\n    cb = fig.colorbar(scatter, ticks=range(10), ax=axes.ravel().tolist())\n    cb.ax.set_title(\"digit\")\n    \n    return fig"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 406}, "colab_type": "code", "id": "_kRdXm2nNOhE", "outputId": "b4d928e9-4296-49c5-c57d-9df1b8e8396c", "tags": []}, "outputs": [], "source": "plot_encodings(train_encodings, train_labels, test_encodings, test_labels)\nplt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nCompare the encodings of the test images with the encodings for regular PCA in exercise 8.1. Are they different and, if yes, in what ways?\n\n*Hint:* Inspect also the range of the axes for $z_1$ and $z_2$ in both plots."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "z4VOBobMjLYm", "tags": []}, "source": "## Decodings\n\nWe can decode the representations in the latent space, according to the definition of the decoding distribution\n\\begin{equation*}\n  p(\\mathbf{x} \\,|\\, \\mathbf{z}) = \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_{784}\\right)\n\\end{equation*}\nin the probabilistic PCA model.\nAgain in contrast to regular PCA, the model provides us with a whole distribution of possible decodings. Analogously to the encodings discussed above, here we take the mean\n\\begin{equation*}\n\\overline{\\mathbf{x}} = \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}\n\\end{equation*}\nof the normal distribution as representative decoding. Of course, alternatively we could sample from the normal distribution defined by the probabilistic PCA model."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nCompute the reconstructions of the test images by mapping the encodings of the test data from the latent space back to the space of images.\n\n*Hint*: It might be easier to work with the equivalent formulation $\\overline{\\mathbf{x}}^\\mathsf{T} = \\mathbf{z}^\\intercal \\mathbf{W}^\\mathsf{T} + \\boldsymbol{\\mu}^\\mathsf{T}$. Note that `mu` is defined as the row vector $\\boldsymbol{\\mu}^\\mathsf{T}$!"}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nAs in regular PCA, it is interesting to compare an image $\\mathbf{x}$ with its reconstruction $\\mathbf{\\tilde{x}}$, to see how much information about $\\mathbf{x}$ is kept/lost by the reduction of $\\mathbf{x}$ to its representation in the latent space. Plot the reconstructions alongside the original images with the function `plot_reconstructions` below. "}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# plot a grid of random pairs of `originals` and `reconstructions`\ndef plot_reconstructions(originals, reconstructions, labels, num_rows=4, num_cols=2):\n    # indices of displayed samples\n    n = originals.shape[0]\n    indices = np.random.choice(n, size=num_rows*num_cols, replace=False)\n\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 10))\n    for (idx, ax) in zip(indices, axes.flat):\n        # extract original, reconstruction, and label\n        original = originals[idx]\n        reconstruction = reconstructions[idx]\n        label = labels[idx]\n\n        # configure subplot\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.grid(False)\n        ax.set_title(f\"Label: {label.item()}\", fontweight='bold')\n\n        # plot original and reconstructed image in a grid\n        grid = np.ones((32, 62))\n        grid[2:30, 2:30] = original.view(28, 28).cpu().numpy()\n        grid[2:30, 32:60] = reconstruction.view(28, 28).cpu().numpy()\n        ax.imshow(grid, vmin=0.0, vmax=1.0, cmap='gray_r')\n\n    return fig"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 611}, "colab_type": "code", "id": "52Tq0RftpsdR", "outputId": "3729a405-3349-43ef-bdec-c1cfec6229ed", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "llasDHlNbMze", "tags": []}, "source": "Moreover, as in exercise 8.1 for regular PCA, we evaluate the average squared reconstruction error\n\\begin{equation*}\n\\mathrm{sqerr} := \\frac{1}{10000} \\sum_{i=1}^{10000} \\|\\mathbf{x}_i - \\tilde{\\mathbf{x}}_i\\|^2_2\n\\end{equation*}\nof the images $\\mathbf{x}_i \\in {[0,1]}^{784}$ and their reconstructions $\\tilde{\\mathbf{x}}_i \\in \\mathbb{R}^{784}$ ($i = 1,\\ldots, 10000$) in the MNIST test data set. It serves as an objective measure for the quality of the reconstructions."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nCompute the average squared reconstruction error."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 611}, "colab_type": "code", "id": "52Tq0RftpsdR", "outputId": "3729a405-3349-43ef-bdec-c1cfec6229ed", "tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "#### Note\n\nAs discussed above, different encodings and decodings could be considered in the probabilistic settings, leading to different reconstructions. The reconstruction that we use is [not optimal in the squared reconstruction error sense](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196), and hence the average squared reconstruction error could be improved by defining it in a different way."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\nNow we have performed exactly the same analysis as for the regular PCA in exercise 8.1. Compare the results you obtained with regular PCA and the probabilistic PCA model, and answer the following questions:\n- Which digits can be reconstructed and decoded quite well, and which ones seem to be more challenging? Are there differences between both approaches?\n- Is the average squared reconstruction error the same as with regular PCA?"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Generating new images\n\n\nIn regular PCA, a priori no distribution of the latent representations is specified by the model. Hence it is difficult to know which encodings might produce reasonably looking images, which makes sampling new encodings and hence new images challenging. In contrast, probabilistic PCA defines the marginal distribution $p(\\mathbf{z})= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2)$. Thus we can generate new MNIST-like images by sampling encodings from this distribution and decoding them."}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "### Task\n\n#### (a)\n\nGenerate 25 images by sampling encodings from $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$ and decoding them. Plot them with the function `plot_images` below.\n\n#### (b)\n\nSometimes the quality of samples can be improved by tuning the so-called temperature of the prior (see, e.g., [this paper](https://arxiv.org/pdf/1807.03039.pdf) and the references therein). Here, sampling with temperature $T > 0$ corresponds to sampling the encodings from $\\mathcal{N}(\\boldsymbol{0}, T^2\\mathbf{I}_2)$ instead of $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$. Hence sampling with temperature $T = 1$ corresponds to sampling from the prior $p_{\\boldsymbol{\\theta}}(\\mathbf{z})$ performed in a). Try different values of $T$ and study how it affects the quality of the samples."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# plot a grid of `images` with `nrows` rows and `ncols` columns\ndef plot_images(images, nrows=5, ncols=5):\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(2 * ncols, 2 * nrows))\n    for image, ax in zip(images, axes.flat):\n        # configure subplot\n        ax.set_xticks(())\n        ax.set_yticks(())\n        ax.grid(False)\n        \n        # plot image\n        ax.imshow(image.view(28, 28).cpu().numpy(),\n                  vmin=0.0, vmax=1.0, cmap='gray_r')\n        \n    return fig"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "## Summary\n\nIn this notebook we trained and analyzed a probabilistic PCA model on MNIST data. Although the PPCA model is more difficult to train than regular PCA, we can generate new MNIST-like images in a general way now! Unfortunately, to be honest, the generated images do not look great. As in regular PCA, we could improve the quality of the reconstructions and samples by increasing the dimension of the latent space. You can try to modify the code accordingly if you want to but it is not a mandatory task."}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "colab": {"name": "Probabilistic PCA.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "lab-apml", "language": "python", "name": "lab-apml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 1}